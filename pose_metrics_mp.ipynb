{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mediapipe inference on static Yoga pose image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723675656.289234    5921 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1723675656.313953    6199 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.183.01), renderer: NVIDIA GeForce RTX 3070/PCIe/SSE2\n"
     ]
    }
   ],
   "source": [
    "# Importing libs for the project\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access dataset\n",
    "def load_coco_annotations(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# OpenCV standard img processing\n",
    "def process_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "    return results\n",
    "\n",
    "# Dataset conversion and processing\n",
    "def coco_keypoints_to_mediapipe(kpts, image_width, image_height):\n",
    "    mediapipe_kpts = []\n",
    "    for i in range(0, len(kpts), 3):\n",
    "        kpx = kpts[i] / image_width\n",
    "        kpy = kpts[i + 1] / image_height\n",
    "        v = kpts[i + 2]  # visibility\n",
    "        #Extracting keypoints\n",
    "        mediapipe_kpts.append((kpx, kpy, v))\n",
    "    return mediapipe_kpts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Object Keypoint Similarity\n",
    "def calculate_oks(y_true, y_pred, bbox_area, sigmas=None):\n",
    "    if sigmas is None:\n",
    "        #Values are taken from COCO database (constants)\n",
    "        sigmas = np.array([0.26, 0.25, 0.25, 0.35, 0.35, 0.79, 0.79, 0.72, 0.72, 0.62, 0.62, 0.79, 0.79, 0.72, 0.72, 0.62, 0.62])\n",
    "    \n",
    "    y_true = np.array(y_true).reshape(-1, 2)\n",
    "    y_pred = np.array(y_pred).reshape(-1, 2)\n",
    "    d = pairwise_distances(y_true, y_pred)  #euclidian distance\n",
    "    # Shows similarity between predicted keypoints and ground truth\n",
    "    oks = np.exp(-d ** 2 / (2 * bbox_area * (sigmas ** 2)))\n",
    "    return np.mean(oks)\n",
    "\n",
    "#Percentage of Correct Parts\n",
    "def calculate_pcp(y_true, y_pred, thresh=0.5):\n",
    "    correct_parts = 0\n",
    "    total_parts = len(y_true) // 2  #A part or full joint is considered as two keypoints\n",
    "    for i in range(total_parts):\n",
    "        if np.linalg.norm(np.array(y_true[i]) - np.array(y_pred[i])) < thresh:\n",
    "            correct_parts += 1\n",
    "    #Evaluates how many full joints are being correctly predicted compared to the entire body\n",
    "    return correct_parts / total_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(coco_json_path, image_dir):\n",
    "    annotations = load_coco_annotations(coco_json_path)\n",
    "    image_id_to_annotations = {ann['image_id']: ann for ann in annotations['annotations']}\n",
    "    oks_scores = []\n",
    "    pcp_scores = []\n",
    "\n",
    "    for image_info in annotations['images']:\n",
    "        image_id = image_info['id']\n",
    "        image_path = os.path.join(image_dir, image_info['file_name'])\n",
    "        image_width = image_info['width']\n",
    "        image_height = image_info['height']\n",
    "\n",
    "        results = process_image(image_path)\n",
    "\n",
    "        # COCO ground truth keypoints\n",
    "        gt_ann = image_id_to_annotations[image_id]\n",
    "        gt_keypoints = coco_keypoints_to_mediapipe(gt_ann['keypoints'], image_width, image_height)\n",
    "\n",
    "        # MediaPipe keypoints\n",
    "        if results.pose_landmarks:\n",
    "            mp_keypoints = []\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                mp_keypoints.append((landmark.x, landmark.y))\n",
    "        else:\n",
    "            mp_keypoints = [(0, 0) for _ in range(len(gt_keypoints))]  # default to zero if no keypoints detected\n",
    "\n",
    "        # Calculate OKS\n",
    "        oks = calculate_oks(gt_keypoints, mp_keypoints, bbox_area=gt_ann['area'])\n",
    "        oks_scores.append(oks)\n",
    "\n",
    "        # Calculate PCP\n",
    "        pcp = calculate_pcp(gt_keypoints, mp_keypoints)\n",
    "        pcp_scores.append(pcp)\n",
    "\n",
    "    return np.mean(oks_scores), np.mean(pcp_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'keypoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m image_test_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myogapose-dataset/yogapose-dataset/test\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#Training and validating\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m oks_train, pcp_train \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoco_train_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_train_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m oks_val, pcp_val \u001b[38;5;241m=\u001b[39m process_dataset(coco_val_json, image_val_dir)\n\u001b[1;32m     13\u001b[0m oks_test, pcp_test \u001b[38;5;241m=\u001b[39m process_dataset(coco_test_json, image_test_dir)\n",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[0;34m(coco_json_path, image_dir)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# COCO ground truth keypoints\u001b[39;00m\n\u001b[1;32m     16\u001b[0m gt_ann \u001b[38;5;241m=\u001b[39m image_id_to_annotations[image_id]\n\u001b[0;32m---> 17\u001b[0m gt_keypoints \u001b[38;5;241m=\u001b[39m coco_keypoints_to_mediapipe(\u001b[43mgt_ann\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeypoints\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, image_width, image_height)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# MediaPipe keypoints\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'keypoints'"
     ]
    }
   ],
   "source": [
    "# Accessing dataset\n",
    "coco_train_json = 'yogapose-dataset/yogapose-dataset/train/_annotations.coco.json'\n",
    "coco_val_json = 'yogapose-dataset/yogapose-dataset/val/_annotations.coco.json'\n",
    "coco_test_json = 'yogapose-dataset/yogapose-dataset/test/_annotations.coco.json'\n",
    "\n",
    "image_train_dir = 'yogapose-dataset/yogapose-dataset/train'\n",
    "image_val_dir = 'yogapose-dataset/yogapose-dataset/val'\n",
    "image_test_dir = 'yogapose-dataset/yogapose-dataset/test'\n",
    "\n",
    "#Training and validating\n",
    "oks_train, pcp_train = process_dataset(coco_train_json, image_train_dir)\n",
    "oks_val, pcp_val = process_dataset(coco_val_json, image_val_dir)\n",
    "oks_test, pcp_test = process_dataset(coco_test_json, image_test_dir)\n",
    "\n",
    "print(f\"Train OKS: {oks_train:.4f}, PCP: {pcp_train:.4f}\")\n",
    "print(f\"Validation OKS: {oks_val:.4f}, PCP: {pcp_val:.4f}\")\n",
    "print(f\"Test OKS: {oks_test:.4f}, PCP: {pcp_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@496.377] global loadsave.cpp:248 findDecoder imread_('yogapose-dataset/yogapose-dataset/train/446_jpg.rf.81efb5629ce52cab0a0f6680e8b68ecf.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myogapose-dataset/yogapose-dataset/train/446_jpg.rf.81efb5629ce52cab0a0f6680e8b68ecf.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with the path to image\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(cv2\u001b[38;5;241m.\u001b[39mimread(image_path), cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(image)\n",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m, in \u001b[0;36mprocess_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_image\u001b[39m(image_path):\n\u001b[1;32m      9\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m---> 10\u001b[0m     image_rgb \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     results \u001b[38;5;241m=\u001b[39m pose\u001b[38;5;241m.\u001b[39mprocess(image_rgb)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_path = 'yogapose-dataset/yogapose-dataset/train/446_jpg.rf.81efb5629ce52cab0a0f6680e8b68ecf.jpg'  # Replace with the path to image\n",
    "results = process_image(image_path)\n",
    "\n",
    "image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "\n",
    "# Add keypoints to the image\n",
    "for landmark in results.pose_landmarks.landmark:\n",
    "    x = int(landmark.x * image.shape[1])\n",
    "    y = int(landmark.y * image.shape[0])\n",
    "    cv2.circle(image, (x, y), 5, (0, 255, 0), -1)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
