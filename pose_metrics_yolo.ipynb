{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv8 inference on static Yoga pose image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load a pretrained YOLO model (recommended for training)\n",
    "model = YOLO('yolov8n-pose.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/rtu/Documents/Metrics_mp/Yoga poses.v4i.yolov8/test/images/test1.jpg: 640x640 1 person, 7.3ms\n",
      "Speed: 19.9ms preprocess, 7.3ms inference, 6598.9ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "yolo_img_path = 'Yoga poses.v4i.yolov8/test/images/test1.jpg'\n",
    "\n",
    "# Perform object detection on an image using the model\n",
    "results = model(yolo_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def keypoint_similarity(gt_kpts, pred_kpts, areas):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        gt_kpts: Ground-truth keypoints, Shape: [M, #kpts, 2],\n",
    "                 where, M is the # of ground truth instances,\n",
    "                        2 in the last dimension denotes coordinates: x,y\n",
    "                         \n",
    "        pred_kpts: Prediction keypoints, Shape: [N, #kpts, 2]\n",
    "                   where  N is the # of predicted instances,\n",
    "\n",
    "        areas: Represent ground truth areas of shape: [M,]\n",
    "\n",
    "    Returns:\n",
    "        oks: The Object Keypoint Similarity (OKS) score tensor of shape: [M, N]\n",
    "    \"\"\"\n",
    "    sigmas = np.array([0.26, 0.25, 0.25, 0.35, 0.35, 0.79, 0.79, 0.72, 0.72, 0.62, 0.62, 0.79, 0.79, 0.72, 0.72, 0.62, 0.62])\n",
    "\n",
    "    # epsilon to take care of div by 0 exception.\n",
    "    EPSILON = torch.finfo(torch.float32).eps\n",
    "    \n",
    "    # Euclidean distance squared:\n",
    "    # d^2 = (x1 - x2)^2 + (y1 - y2)^2\n",
    "    # Shape: (M, N, #kpts)\n",
    "    dist_sq = (gt_kpts[:, None, :, 0] - pred_kpts[..., 0])**2 + (gt_kpts[:, None, :, 1] - pred_kpts[..., 1])**2\n",
    "\n",
    "    # COCO assigns k = 2σ.\n",
    "    k = 2 * sigmas\n",
    "\n",
    "    # Denominator in the exponent term. Shape: [M, 1, #kpts]\n",
    "    denom = 2 * (k**2) * (areas[:, None, None] + EPSILON)\n",
    "\n",
    "    # Exponent term. Shape: [M, N, #kpts]\n",
    "    exp_term = dist_sq / denom\n",
    "\n",
    "    # Object Keypoint Similarity. Shape: (M, N)\n",
    "    oks = torch.exp(-exp_term).mean(-1)\n",
    "\n",
    "    return oks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[        344         327]\n",
      " [      546.5       564.5]\n",
      " [     285.45      226.36]\n",
      " [     256.36      275.45]\n",
      " [     311.82      280.91]\n",
      " [        320      380.91]\n",
      " [     275.45         390]\n",
      " [     151.82      391.82]\n",
      " [     397.27      458.18]\n",
      " [     163.64      494.55]\n",
      " [     530.91      490.91]\n",
      " [     298.18      184.55]\n",
      " [     330.91      114.55]\n",
      " [     277.27      188.18]\n",
      " [     325.45      110.91]]\n",
      "[(267, 206), (316, 245), (303, 241), (306, 379), (266, 376), (397, 426), (172, 413), (542, 500), (174, 485), (308, 231), (312, 228), (305, 152), (284, 151)]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (15) must match the size of tensor b (13) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 58\u001b[0m\n\u001b[1;32m     53\u001b[0m selected_keypoints \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(selected_keypoints)\n\u001b[1;32m     56\u001b[0m areas \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m640\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m640\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.53\u001b[39m])\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mkeypoint_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_kpts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_keypoints\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mareas\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mfor r in results:    \u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    # Load and display the image\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 27\u001b[0m, in \u001b[0;36mkeypoint_similarity\u001b[0;34m(gt_kpts, pred_kpts, areas)\u001b[0m\n\u001b[1;32m     22\u001b[0m EPSILON \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39meps\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Euclidean distance squared:\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# d^2 = (x1 - x2)^2 + (y1 - y2)^2\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Shape: (M, N, #kpts)\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m dist_sq \u001b[38;5;241m=\u001b[39m (\u001b[43mgt_kpts\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpred_kpts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m (gt_kpts[:, \u001b[38;5;28;01mNone\u001b[39;00m, :, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m pred_kpts[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# COCO assigns k = 2σ.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m sigmas\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (15) must match the size of tensor b (13) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# Define the indices for the 13 keypoints we need\n",
    "# Using MediaPipe indices: \n",
    "# 0: Nose, 5: Left Shoulder, 6: Right Shoulder, 11: Left Hip, 12: Right Hip, \n",
    "# 13: Left Knee, 14: Right Knee, 15: Left Ankle, 16: Right Ankle, 7: Left Elbow, 8: Right Elbow,\n",
    "# 9: Left Wrist, 10: Right Wrist\n",
    "selected_indices = [0, 5, 6, 11, 12, 13, 14, 15, 16, 7, 8, 9, 10]\n",
    "\n",
    "\n",
    "# Predicted keypoints----------------------------------\n",
    "denormalized_kps = []\n",
    "\n",
    "# Process results\n",
    "for r in results:\n",
    "    keypoints = r.keypoints.xyn.cpu().numpy()  # Normalized keypoints (x, y, conf)\n",
    "    \n",
    "    for kp in keypoints[0]:\n",
    "            x, y = int(kp[0] * 640), int(kp[1] * 640) # denormalize , if needed\n",
    "            denormalized_kps.append((x,y))\n",
    "\n",
    "selected_keypoints = []\n",
    "\n",
    "# Filter for the 13 specific keypoints\n",
    "for i in selected_indices:\n",
    "    selected_keypoints.append(denormalized_kps[i])\n",
    "#----------------------------------------------------\n",
    "\n",
    "\n",
    "#Ground truth keypoints+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "# File path to your YOLO txt file\n",
    "file_path = 'Yoga poses.v4i.yolov8/test/labels/1_123_jpg.rf.aec8214c1ba57eef43d571faa5775f8a.txt'\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, 'r') as file:\n",
    "    data = file.read()\n",
    "\n",
    "normal_gt_kpts = [float(value) for i, value in enumerate(data.split()) if 0 < float(value) <= 1]\n",
    "\n",
    "\n",
    "# Reshape keypoints into pairs of (x, y)\n",
    "normal_gt_kpts = np.array(normal_gt_kpts).reshape(-1, 2)\n",
    "\n",
    "# Denormalize keypoints\n",
    "gt_kpts = np.zeros_like(normal_gt_kpts)\n",
    "gt_kpts[:, 0] = normal_gt_kpts[:, 0] * 640   # Denormalize x by image width\n",
    "gt_kpts[:, 1] = normal_gt_kpts[:, 1] * 640  # Denormalize y by image height\n",
    "\n",
    "\n",
    "print(gt_kpts)\n",
    "print(selected_keypoints)\n",
    "#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "gt_kpts = torch.tensor(gt_kpts)\n",
    "selected_keypoints = torch.tensor(selected_keypoints)\n",
    "\n",
    "\n",
    "areas = torch.tensor([640*640*0.53])\n",
    "\n",
    "print(keypoint_similarity(gt_kpts.unsqueeze(0), selected_keypoints.unsqueeze(0), areas))\n",
    "\n",
    "\"\"\"\n",
    "for r in results:    \n",
    "    # Load and display the image\n",
    "    image = cv2.imread(yolo_img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Plot the image and the selected keypoints\n",
    "    plt.imshow(image)\n",
    "\n",
    "    for kp in selected_keypoints:\n",
    "        cv2.circle(image, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Extract the pose landmarks from the results\n",
    "for result in results:\n",
    "    if result.keypoints is not None:\n",
    "        # Access keypoints tensor directly\n",
    "        keypoints = result.keypoints[0]  # Assume the first detection (or iterate over multiple detections)\n",
    "\n",
    "        # Filter for the 13 specific keypoints\n",
    "        selected_keypoints = keypoints[selected_indices]\n",
    "\n",
    "        # Load and display the image\n",
    "        image = cv2.imread(yolo_img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Plot the image and the selected keypoints\n",
    "        plt.imshow(image)\n",
    "\n",
    "        for kp in selected_keypoints:\n",
    "            x, y, confidence = kp[:3]  # Assumes that each kp has three values: x, y, confidence\n",
    "            if confidence > 0.5:  # Filter by confidence if needed\n",
    "                \n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
